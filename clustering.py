# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cMzhCVgun-qr1-JjoGDBlTeuH5Rsycox

# 21.Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot
"""

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

x, y = make_blobs(n_samples=1000, centers=4, random_state=42)

# split the train and test data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# train the xmodel
model = KMeans(n_clusters=4, random_state=42)
model.fit(x_train)

# predict the model
y_pred = model.predict(x_test)

# visualize the model
plt.scatter(x_test[:, 0], x_test[:, 1], c=y_pred)
plt.show()

model = KMeans(n_clusters=4, random_state=42)
model.fit(x_train)

# using centroid
centers = model.cluster_centers_
plt.scatter(x_test[:, 0], x_test[:, 1], c=y_pred)
plt.scatter(centers[:, 0], centers[:, 1], marker='*', c='red')
plt.show()

"""# 22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels"""

from sklearn.datasets import load_iris
from sklearn.cluster import AgglomerativeClustering
from sklearn.model_selection import train_test_split

irsi = load_iris()
x = irsi.data
y = irsi.target

# split the train and test data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# train the model
model = AgglomerativeClustering(n_clusters=3)
label = model.fit_predict(x_train)

# print the 10 labes
print("First 10 predicted cluster labels:")
print(label[:10])

"""# 23.Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot"""

from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

x , y = make_moons(n_samples=1000, noise=0.05, random_state=42)

# split the train and test data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# train the model
dbscan = DBSCAN(eps=0.2, min_samples=5)
labels = dbscan.fit_predict(x_train)

# predict the model
y_pred = model.fit_predict(x_test)

# Core/cluster points
plt.scatter(x_train[labels != -1, 0], x_train[labels != -1, 1],
            c=labels[labels != -1], cmap='plasma', s=50, label='Clusters')

plt.scatter(x_train[labels == -1, 0], x_train[labels == -1, 1],
            c='black', marker='x', s=80, label='Outliers')

plt.title("DBSCAN Clustering on make_moons Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.grid(True)
plt.show()

"""# 24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster"""

from sklearn.datasets import load_wine
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

# load the dataset
wine = load_wine()
x = wine.data

# scale the model
scaler = StandardScaler()
x = scaler.fit_transform(x)

# Step 3: Apply K-Means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(x)

# Step 4: Print the size of each cluster
unique, counts = np.unique(labels, return_counts=True)
print("Size of each cluster:")
for cluster_id, size in zip(unique, counts):
    print(f"Cluster {cluster_id}: {size} samples")

"""# 25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result"""

from sklearn.datasets import make_circles
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

x,y = make_circles(n_samples=1000 , noise=0.5, factor=0.5, random_state=42)

# apply dbscan
dbscan = DBSCAN(eps=0.1, min_samples=5)
labels = dbscan.fit_predict(x)

# plot the chart
plt.figure(figsize=(8, 6))
unique_labels = set(labels)
colors = plt.cm.jet(np.linspace(0, 1, len(unique_labels)))

for label, color in zip(unique_labels, colors):
    if label == -1:
        # Black used for noise.
        color = 'k'
        label_text = "Noise"
    else:
        label_text = f"Cluster {label}"

    plt.scatter(x[labels == label, 0], x[labels == label, 1],
                c=[color], label=label_text, s=20, edgecolors='k')

plt.title("DBSCAN Clustering on make_circles data")
plt.xlabel("X1")
plt.ylabel("X2")
plt.legend()
plt.grid(True)
plt.show()

"""# 26.Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids"""

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# load the dataset
cancer = load_breast_cancer()
x = cancer.data
feature_names = cancer.feature_names

# scale the model
scaler = MinMaxScaler()
x = scaler.fit_transform(x)

# train the model
model = KMeans(n_clusters=2, random_state=42)
model.fit(x)

# create centroid in the model
centers = model.cluster_centers_

# plot the chart
plt.figure(figsize=(8, 6))
plt.scatter(x[:, 0], x[:, 1], c=model.labels_, cmap='rainbow',s=50, edgecolors='k')
plt.scatter(centers[:, 0], centers[:, 1], c='yellow', marker='*', s=200, label='Centroids')
plt.title("K-Means Clustering on Breast Cancer Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.grid(True)
plt.show()

# Convert centroids to DataFrame for better readability
centroids_df = pd.DataFrame(centers, columns=feature_names)
print("Cluster Centroids:\n")
print(centroids_df)

"""# 27.4 Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN"""

from sklearn.datasets import make_blobs
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the data
X,Y = make_blobs(n_samples=1000, centers=4, cluster_std=[0.5, 1.0, 1.5, 2.0], random_state=42)

# train the model
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(x)

# 3. Plot the result
plt.figure(figsize=(8, 6))
unique_labels = set(labels)
colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))

for label, color in zip(unique_labels, colors):
    if label == -1:
        color = 'k'
        label_text = "Noise"
    else:
        label_text = f"Cluster {label}"

    plt.scatter(X[labels == label, 0], X[labels == label, 1],
                c=[color], label=label_text, s=30, edgecolors='k')

plt.title("DBSCAN Clustering on make_blobs with Varying Std Dev")
plt.xlabel("X1")
plt.ylabel("X2")
plt.legend()
plt.grid(True)
plt.show()

"""# 28.Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means"""

from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the dataset
df = load_digits()
x = df.data
y = df.target

# use PCA
pca = PCA(n_components=2)
x_pca = pca.fit_transform(x)

# train the model using k means
kmeans = KMeans(n_clusters=10, random_state=42)
labels = kmeans.fit_predict(x_pca)

# find the centorid
centers = kmeans.cluster_centers_

# plot the chart
plt.figure(figsize=(8, 6))
plt.scatter(x_pca[:, 0], x_pca[:, 1], c=labels, cmap='rainbow', s=50, edgecolors='k')
plt.scatter(centers[:, 0], centers[:, 1], c='yellow', marker='*', s=200, label='Centroids')
plt.title("K-Means Clustering on Digits Dataset (PCA Reduced)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

"""# 29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart"""

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

# load the dataset
x, y = make_blobs(n_samples=1000, centers=4, random_state=42)

silhouette_score_dict = {}
for k in range(2, 6):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(x)
    # Call the imported silhouette_score function
    score = silhouette_score(x, labels)
    silhouette_scores_dict[k] = score

# Step 4: Plot silhouette scores as a bar chart
plt.figure(figsize=(8, 6))
plt.bar(silhouette_scores_dict.keys(), silhouette_scores_dict.values(), color='skyblue')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Scores for k = 2 to 5")
plt.xticks(list(silhouette_scores_dict.keys()))
plt.grid(axis='y')
plt.tight_layout()
plt.show()

"""# 30.Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage."""

from sklearn.datasets import load_iris
from scipy.cluster.hierarchy import dendrogram , linkage
import matplotlib.pyplot as plt

# load the dataset
iris = load_iris()
x = iris.data
y= iris.target

# compute with linkage
linkage_matrix = linkage(x, method='average')

# plot the dendrogram
plt.figure(figsize=(10, 6))
dendrogram(linkage_matrix, labels=y, leaf_rotation=90, leaf_font_size=10)
plt.title("Dendrogram")
plt.xlabel("Samples")
plt.ylabel("Distance")
plt.show()

"""# 31.Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries"""

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from matplotlib.colors import ListedColormap
import numpy as np

# load the dataset
X , y = make_blobs(n_samples=1000, centers=2, cluster_std=[1.0, 2.0], random_state=42)

# train the model
model = KMeans(n_clusters=2, random_state=42)
model.fit(X)

labels = model.predict(X)
center = model.cluster_centers_

# Step 3: Create a mesh grid for decision boundaries
h = 0.1  # step size in the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
grid_points = np.c_[xx.ravel(), yy.ravel()]
Z = kmeans.predict(grid_points)
Z = Z.reshape(xx.shape)

# Step 4: Plot decision boundaries and data
plt.figure(figsize=(10, 7))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FFCCCC', '#CCFFCC', '#CCCCFF']))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis', edgecolor='k')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, marker='X', label='Centroids')
plt.title("K-Means Clustering with Decision Boundaries (Overlapping Clusters)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# 32.Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results"""

from sklearn.datasets import load_digits
from sklearn.cluster import DBSCAN
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

digit = load_digits()
x = digit.data
y = digit.target

# transform the to scaler
scaler = StandardScaler()
x = scaler.fit_transform(x)

# 3. Reduce dimensions with t-SNE (to 2D for visualization)
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(x)

# train the model
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X_tsne)

# 5. Visualize the results
plt.figure(figsize=(10, 7))
unique_labels = set(labels)
colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))

for label, color in zip(unique_labels, colors):
    mask = labels == label
    plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=[color], label=f'Cluster {label}' if label != -1 else 'Noise', s=30)

plt.title('DBSCAN Clustering on Digits Dataset (after t-SNE)')
plt.legend()
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.grid(True)
plt.show()

"""# 33.Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result"""

from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

x,y = make_blobs(n_samples=1000, n_features=2, centers=3, random_state=42)

# train with the model
model = AgglomerativeClustering(n_clusters=3, linkage='complete')
labels = model.fit_predict(x)



# 3. Plot the result
plt.figure(figsize=(8, 6))
plt.scatter(x[:, 0], x[:, 1], c=labels, cmap='rainbow', s=40)
plt.title("Agglomerative Clustering with Complete Linkage")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.grid(True)
plt.show()

"""# 34.Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot"""

from sklearn.datasets import load_breast_cancer
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler # Corrected spelling here
import numpy as np

# load the dataset
cancer = load_breast_cancer()
x = cancer.data
y= cancer.target

# scaler the data
scaler = StandardScaler()
x = scaler.fit_transform(x)

# train the model
inertia_values = []
for k in range(2, 7):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(x)
    inertia_values.append(kmeans.inertia_)

# plot the chart
plt.figure(figsize=(10, 6))
plt.plot(range(2, 7), inertia_values, marker='o', linestyle='-', color='b')
plt.title("Inertia Values for K-Means Clustering")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.grid(True)
plt.show()

"""# 35.A Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage"""

from sklearn.datasets import make_circles
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the data
x,y = make_circles(n_samples=1000, noise=0.05,random_state=42)

# train the model
model = AgglomerativeClustering(n_clusters=2, linkage='single')
labels = model.fit_predict(x)

# plot the chart
plt.figure(figsize=(8, 6))
plt.scatter(x[:, 0], x[:, 1], c=labels, cmap='rainbow', s=40)
plt.title("Agglomerative Clustering with Single Linkage")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.grid(True)
plt.show()

"""# 36.Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)."""

from sklearn.datasets import load_wine
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load wine dataset
wine = load_wine()
x = wine.data
y = wine.target

# scale the data
scaler = StandardScaler()
x = scaler.fit_transform(x)

# train the model
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(x)

# count the number of cluster
num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
print(f"Number of clusters (excluding noise): {num_clusters}")

# plot the chart
plt.figure(figsize=(8, 6))
plt.plot(x[:, 0], x[:, 1], 'o', markersize=5, color='blue', alpha=0.7)
plt.title("Wine Dataset")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.grid(True)
plt.show()

"""# 37.Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points"""

from sklearn.datasets import make_blobs

from sklearn.cluster import KMeans
import pandas as pd
import matplotlib.pyplot as plt

# generate the data
x,y = make_blobs(n_samples=1000, centers=3, random_state=42)

# train the data
kmeans = KMeans()
labels = kmeans.fit_predict(x)

# total center
center = kmeans.cluster_centers_

# plot the chart
plt.figure(figsize=(8, 6))
plt.scatter(x[:, 0], x[:, 1], c=labels, cmap='rainbow', s=40)
plt.scatter(center[:, 0], center[:, 1], c='green', marker='X', s=200, label='Cluster Centers')
plt.title("K-Means Clustering with Cluster Centers")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.grid(True)
plt.show()
#

"""# 38.Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise"""

from sklearn.datasets import load_iris
from sklearn.cluster import  DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the dataset
iris = load_iris()
x = iris.data
y = iris.target

# Scale the data
scaler = StandardScaler()
x = scaler.fit_transform(x)

# train the model
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(x)

# 4. Count noise samples (label == -1)
n_noise = np.sum(labels == -1)

# 5. Output the result
print(f"Number of noise samples: {n_noise}")

"""# 39.Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result"""

from sklearn.datasets import make_moons
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the dataset
x,y = make_moons(n_samples=1000, noise=0.05, random_state=42)

# train the models
kmeans = KMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(x)

# viualize the model
plt.figure(figsize=(8, 6))
plt.scatter(x[:, 0], x[:, 1], c=labels, cmap='rainbow', s=40)
plt.title("K-Means Clustering on make_moons")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.grid(True)
plt.show()

"""# 40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot."""

from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the dataset
digit = load_digits()
x = digit.data

# make PCA
pca = PCA(n_components=3)
pca.fit_transform(x)

# train the models
kmeans = KMeans(n_clusters=10, random_state=42)
labels = kmeans.fit_predict(x)

# 5. 3D Scatter Plot
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(x[:, 0], x[:, 1], x[:, 2], c=labels, cmap='tab10', s=30)
ax.set_title('KMeans Clustering on Digits Dataset (3D PCA)')
ax.set_xlabel('PCA 1')
ax.set_ylabel('PCA 2')
ax.set_zlabel('PCA 3')
plt.colorbar(scatter, ax=ax, label='Cluster Label')
plt.show()

"""# 41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering"""

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the datasets
x,y = make_blobs(n_samples=1000, centers=5, random_state=42)

# split train and test data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

Kmenas = KMeans(n_clusters=5, random_state=42)
labels = Kmenas.fit_predict(x_train)

center = Kmenas.cluster_centers_

score = silhouette_score(x_train, labels)
print(f"Silhouette Score: {score}")

# plot the chart
plt.figure(figsize=(8, 6))
plt.scatter(x_train[:, 0], x_train[:, 1], c=labels, cmap='rainbow', s=40)
plt.scatter(center[:, 0], center[:, 1], c='green', marker='X', s=200, label='Cluster Centers')
plt.title("K-Means Clustering with Cluster Centers")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.grid(True)
plt.show()

"""# 42.Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D"""

from sklearn.datasets import load_breast_cancer
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the datasets
cancer = load_breast_cancer()
x = cancer.data
y = cancer.target

# split train and test data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# apply PCA
pca = PCA(n_components=2)
x_train_pca = pca.fit_transform(x_train)

# train the model
model = AgglomerativeClustering(n_clusters=2)
labels = model.fit_predict(x_train_pca)

# plot 2D chart
plt.figure(figsize=(8, 6))
plt.scatter(x_train_pca[:, 0], x_train_pca[:, 1], c=labels, cmap='rainbow', s=40)
plt.title("Agglomerative Clustering on Breast Cancer Dataset (2D PCA)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid(True)
plt.show()

"""# 43.Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side"""

from sklearn.datasets import make_circles
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the data
x,y = make_circles(n_samples=1000, noise=0.05, random_state=42)

# scaled the data
scaler = StandardScaler()
x = scaler.fit_transform(x)

# train with Kmeans model
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans_labels = kmeans.fit_predict(x)

# train with DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(x)

# 4. Visualize the results side-by-side
fig, axs = plt.subplots(1, 2, figsize=(12, 5))
 # KMeans Plot
axs[0].scatter(x[:, 0], x[:, 1], c=kmeans_labels, cmap='viridis', s=50)
axs[0].set_title("KMeans Clustering")

# DBSCAN Plot
# Note: DBSCAN labels noise as -1 (will appear as a separate color)
axs[1].scatter(x[:, 0], x[:, 1], c=dbscan_labels, cmap='viridis', s=50)
axs[1].set_title("DBSCAN Clustering")

plt.show()

"""# 44.Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering"""

from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score , silhouette_samples
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the datasets
iris = load_iris()
x= iris.data
y= iris.target

# train the model
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(x)

# silhouette soure
silhouette_avg = silhouette_score(x, labels)
print(f"Silhouette Score: {silhouette_avg}")

# 3. Calculate Silhouette Coefficients for each sample
silhouette_vals = silhouette_samples(x, labels)
avg_score = silhouette_score(x, labels)

#4. Plot Silhouette Coefficient for each sample
plt.figure(figsize=(10, 6))
y_lower = 10

for i in range(3):  # Loop through each cluster
    cluster_silhouette_vals = silhouette_vals[labels == i]
    cluster_silhouette_vals.sort()
    size_cluster_i = cluster_silhouette_vals.shape[0]
    y_upper = y_lower + size_cluster_i

    plt.fill_betweenx(np.arange(y_lower, y_upper),
                      0, cluster_silhouette_vals,
                      alpha=0.7, label=f'Cluster {i}')

    y_lower = y_upper + 10  # space between clusters

plt.axvline(x=avg_score, color='red', linestyle='--', label='Average Silhouette Score')
plt.xlabel('Silhouette Coefficient')
plt.ylabel('Samples')
plt.title('Silhouette Plot for Iris Clustering (KMeans)')
plt.legend()
plt.tight_layout()
plt.show()

"""# 45.. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters"""

from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

# load the datasets
x,y = make_blobs(n_samples=1000, centers=3, random_state=42)

# train the model
model = AgglomerativeClustering(n_clusters=3, linkage='average')
labels = model.fit_predict(x)

# visualize clusters
plt.figure(figsize=(8, 6))
plt.scatter(x[:, 0], x[:, 1], c=labels, cmap='rainbow', s=40)
plt.title("Agglomerative Clustering with Average Linkage")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.grid(True)
plt.show()

"""# 46.. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)"""

from sklearn.datasets import load_wine
from sklearn.cluster import KMeans
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

# load the data
wine = load_wine()
X = pd.DataFrame(wine.data, columns=wine.feature_names)
y= wine.target

center = kmeans.cluster_centers_

X_subset = X.iloc[:, :4]

# 2. Apply KMeans clustering (let's choose 3 clusters based on known classes)
kmeans = KMeans(n_clusters=3, random_state=42)
X_subset['Cluster'] = kmeans.fit_predict(X_subset)

# 3. Create a Seaborn pairplot with cluster coloring
sns.pairplot(X_subset, hue='Cluster', palette='viridis', diag_kind='kde', corner=True)
plt.suptitle('KMeans Clustering on Wine Dataset (First 4 Features)', y=1.02)
plt.show()

"""# 47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count"""

from sklearn.datasets import make_blobs
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

x,y = make_blobs(n_samples=1000, centers=2, random_state=42)

# train the model
DBSCAN = DBSCAN(eps=0.5, min_samples=5)
labels = DBSCAN.fit_predict(x)

# 3. Count clusters and noise
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # -1 = noise
n_noise = np.sum(labels == -1)

print(f"Number of clusters found: {n_clusters}")
print(f"Number of noise points: {n_noise}")



# 4. Visualize the clusters and noise
plt.figure(figsize=(8, 6))
plt.scatter(x[:, 0], x[:, 1], c=labels, cmap='Paired', s=50)
plt.title('DBSCAN Clustering on Noisy Blobs')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

"""# 48.Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters."""

from sklearn.datasets import load_digits
from sklearn.manifold import TSNE
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the Digits dataset
digits = load_digits()
X = digits.data
y_true = digits.target

# 2. Reduce dimensions to 2D using t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X)

# 3. Apply Agglomerative Clustering (let’s use 10 clusters, matching digits 0-9)
agglo = AgglomerativeClustering(n_clusters=10)
labels = agglo.fit_predict(X_tsne)

# 4. Plot the clusters
plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels, palette='tab10', s=50, legend='full')
plt.title('Agglomerative Clustering on Digits Dataset (t-SNE Reduced)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.legend(title='Cluster')
plt.show()